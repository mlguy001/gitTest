# -*- coding: utf-8 -*-
"""Projet Rating ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_jYx-xmTjlLxBCZVv7mbQqTBOlkGCvK4
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

path = "https://raw.githubusercontent.com/mlguy001/gitTest/main/corporate_rating.csv"
df = pd.read_csv(path)

rank = {'D':0,'C':1,'CC':2,'CCC':3,'B':4,'BB':5,'BBB':6,'A':7,'AA':8,'AAA':9}
order = pd.DataFrame()
order["Rank"] = rank.values()
order.index = rank.keys()
order = pd.merge(order, df["Rating"].value_counts().rename("Frequency"), left_index=True, right_index=True, how="left")
order["Frequency"] = order["Frequency"].cumsum()

orderType = "Rank"
orderType = "Frequency"
df["Score"] = df["Rating"].map(order[orderType])

"""**Distributions**"""

col1 = "Sector"
col2 = "Rating Agency Name"
table = pd.pivot_table(df[[col1, col2]], index=[col1],
                       columns=[col2], aggfunc=len, fill_value=0)
sns.heatmap(table)

sns.catplot(x="Rating", kind="count", palette="ch:.25", data=df, order=rank)

col = col2
dist = df.groupby([col, "Rating"]).agg({
    col : lambda x:x.count()/df[df[col]==x.iloc[0]][col2].count()
})
dist = dist.rename(columns={col:"Rate"})
dist.reset_index(inplace=True)

l = dist[col].unique()
N = len(l)
fig, axes = plt.subplots(nrows=N, sharex=True, figsize=(5, N*2.5))
fig.suptitle(f"Distributions per {col}")
for i in range(N):
  data = dist[dist[col]==l[i]]
  sns.barplot(ax=axes[i], y="Rate", x="Rating", order=rank, data=data)
  if i!= N-1:
    axes[i].axes.get_xaxis().set_visible(False)
  axes[i].set_title(l[i])

"""**Linear relationships**"""

for col in df.describe().columns:
  sup = df[col].quantile(0.99)
  inf = df[col].quantile(0.01)
  data = df[(inf<=df[col]) & (df[col]<=sup)]
  data = data[["Score", col]]
  data.plot.scatter(x=col, y="Score")

"""**Outlier handling**"""

for col in df.describe().columns:
  sup = df[col].quantile(0.99)
  inf = df[col].quantile(0.01)
  df[col] = df[col].apply(lambda x:max(x,inf))
  df[col] = df[col].apply(lambda x:min(x,sup))

"""**Feature selection**"""

corr = df.corr()["Score"]
numericFeatures = list(corr[abs(corr) >= 0.05].index[:-1])
numericFeatures

numericFeatures = df.describe().columns[:-1]
numericFeatures

"""**Preprocessing**"""

from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

X_numeric = df[numericFeatures].values

scaler = MinMaxScaler(feature_range=(0, 1))
X_numeric = scaler.fit_transform(X_numeric)

categoricalFeatures = ["Rating Agency Name", "Sector"]
encoder = OneHotEncoder()
X_categorical = encoder.fit_transform(df[categoricalFeatures]).toarray()

X = np.hstack([X_numeric, X_categorical])
Y = df["Rating"].values

"""**Imbalance handling - Oversampling**"""

from imblearn.over_sampling import RandomOverSampler

oversampler = RandomOverSampler()
X,Y = oversampler.fit_resample(X,Y)

print("X:", X.shape)
print("Y:", Y.shape)

data = pd.DataFrame({"Rating":Y})
sns.catplot(x="Rating", kind="count", palette="ch:.25", data=data, order=rank)

"""**Classifier**"""

from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)

model = RandomForestClassifier()

model.fit(X_train, Y_train)

predicted = model.predict(X_test)
print(f"Training accuracy score = {accuracy_score(Y_train, model.predict(X_train))}")
print(f"Test accuracy score = {accuracy_score(Y_test, predicted)}")

# print("Test Confusion matrix\n", confusion_matrix(Y_test, predicted))
print("Test Classification report\n", classification_report(Y_test, predicted))

# ConfusionMatrixDisplay.from_estimator(model, X_test, Y_test)
ConfusionMatrixDisplay.from_predictions(Y_test, predicted, labels=list(rank.keys()))

"""**Neural Network**"""

from keras.models import Sequential
from keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD, Adam, Adamax, Adagrad, RMSprop
from tensorflow.keras.initializers import RandomNormal
import tensorflow as tf

y = pd.get_dummies(Y)
y = y[rank.keys()].values

X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

def scheduler(epoch, lr):
  if epoch<100:
    return lr
  else:
    return lr*0.99

callback = tf.keras.callbacks.LearningRateScheduler(scheduler)

model = Sequential()
model.add(Dense(16, activation='relu', input_dim=X.shape[1], kernel_initializer=RandomNormal(stddev=1, seed=40)))
# model.add(Dropout(0.1))
model.add(Dense(8, activation='relu', kernel_initializer=RandomNormal(stddev=1, seed=40)))
model.add(Dense(y.shape[1], activation='softmax'))

epochs = 300
learning_rate = 0.01
momentum = 0.1

optimizer = RMSprop(learning_rate=learning_rate, momentum=momentum)

loss = 'categorical_crossentropy'
loss = 'mean_squared_error'

model.compile(optimizer=optimizer,
              loss=loss,
              metrics="accuracy")

model.summary()

history = model.fit(X_train, Y_train, epochs=epochs
                    , callbacks=[callback]
                    )
# history = model.fit(X_train, Y_train, epochs=epochs, validation_split=0.2, batch_size=50)

metric = 'loss'
metric = 'accuracy'

plt.plot(history.history[metric])
# plt.plot(history.history[f"val_{metric}"])
plt.title(metric)
plt.ylabel(metric)
plt.xlabel('epoch')
plt.legend(["train", "test"], loc="upper left")

scores = model.evaluate(X_test, Y_test, verbose=0)
print("Evaluation score", scores)

predicted = model.predict(X_test)
testActual, testPredicted = np.argmax(Y_test, axis=1), np.argmax(predicted, axis=1)
trainActual, trainPredicted = np.argmax(Y_train, axis=1), np.argmax(model.predict(X_train), axis=1)

print(f"Training accuracy score = {accuracy_score(trainActual, trainPredicted)}")
print(f"Test accuracy score = {accuracy_score(testActual, testPredicted)}")

# print("Test Confusion matrix\n", confusion_matrix(testActual, testPredicted))
print("Test Classification report\n", classification_report(testActual, testPredicted))

ConfusionMatrixDisplay.from_predictions(testActual, testPredicted, display_labels=list(rank.keys()))

"""**Linear regression**"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.linear_model import ElasticNet, BayesianRidge

orderType = "Rank"
# orderType = "Frequency"
df["Score"] = df["Rating"].map(order[orderType])
order["Threshold"] = (order[orderType].shift(-1) + order[orderType])/2

Y_scores = np.vectorize(lambda x:order[orderType].loc[x])(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y_scores, test_size=0.2, shuffle=True)

ratings = list(order.index)
thresholds = list(order["Threshold"])
thresholds[-1] = np.Infinity
def decideRating(score):
  for r,t in zip(ratings, thresholds):
    res = r
    if not t:
      return ratings[-1]
    elif score < t:
      return res

# model = ElasticNet(fit_intercept=False)
model = LinearRegression(fit_intercept=False)

model.fit(X_train, Y_train)

predicted = model.predict(X_test)
predictedLabels = np.vectorize(decideRating)(predicted)
reconstructedActualLabels = np.vectorize(decideRating)(Y_test)

predictedTrain = model.predict(X_train)
predictedTrainLabels = np.vectorize(decideRating)(predictedTrain)
reconstructedActualTrainLabels = np.vectorize(decideRating)(Y_train)

print(f"Train Labels accuracy score = {accuracy_score(reconstructedActualTrainLabels, predictedTrainLabels)}")
print(f"Train Scores RMS = {np.sqrt(np.sum((Y_train-predictedTrain)))}")

print(f"Test Labels accuracy score = {accuracy_score(reconstructedActualLabels, predictedLabels)}")
print(f"Test Scores RMS = {np.sqrt(np.sum((Y_test-predicted)))}")

print("Test Classification report\n", classification_report(reconstructedActualLabels, predictedLabels))

print("Train confusion matrix")
ConfusionMatrixDisplay.from_predictions(reconstructedActualTrainLabels, predictedTrainLabels, display_labels=list(rank.keys()))

print("Test confusion matrix")
ConfusionMatrixDisplay.from_predictions(reconstructedActualLabels, predictedLabels, display_labels=list(rank.keys()))

model.coef_

model.intercept_